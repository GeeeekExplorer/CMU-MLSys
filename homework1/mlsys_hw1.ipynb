{"cells":[{"cell_type":"markdown","metadata":{"id":"CPrCLeqDcia_"},"source":["# Machine Learning Systems Homework 1\n","\n","<a target=\"_blank\" href=\"https://colab.research.google.com/github/mlsyscourse/homework1/blob/main/mlsys_hw1.ipynb\">\n","  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n","</a>\n","\n","**Homework due: Feb 12, 2024, 11:59 pm, Eastern Time**.\n","\n","Automatic differentiation is the foundation technique of training a machine learning model.\n","In this homework, you will implement a simple prototype automatic differentiation system (learned in lecture 4), build up your own logistics regression model, and train the model on a handwritten digit dataset.\n","\n","* You should work on this homework **individually** -- it is not a team homework.\n","* This homework does not require GPU. You can do the homework on either Google Colab (by clicking the badge above), your laptop/desktop, or any server that you have access to.\n","* This homework is pure Python. No C++ is needed in this homework.\n","* Please check out the end of this notebook for the homework submission requirement.\n","* Please do not share your solution on publicly available websites (e.g., GitHub).\n","* **About testing and grading.** We provide a set of public test cases (under `tests/` directory) for you to test your homework implementation. **Meanwhile, we have a set of private test cases to grade your homework.** The scores you get in each task is proportional to the number of private test cases you pass. You can submit your homework for multiple times, and only the last submission will be graded after the homework due. **This means you will not be able to see your scores immediately after submission.** We encourage you to create your own test cases, which helps you confirm the correctness of your code."]},{"cell_type":"markdown","metadata":{"id":"NDVuWaftK_h3"},"source":["## Preparation\n","\n","* If you are using Google Colab environment, please make a copy of this notebook file by selecting \"Save a copy in Drive\" from the \"File\" menu, and then run the code block below to set up workspace. After cloning, you will see the cloned repository in the \"Files\" bar on the left."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2864,"status":"ok","timestamp":1706413992841,"user":{"displayName":"Ruihang Lai","userId":"03605576729730874720"},"user_tz":300},"id":"hx2QcOuFR6Nu","outputId":"dd2dccc8-63fe-424b-c2cd-5077321470a2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive\n","/content/drive/MyDrive/15442\n","Cloning into 'homework1'...\n","remote: Enumerating objects: 105, done.\u001b[K\n","remote: Counting objects: 100% (105/105), done.\u001b[K\n","remote: Compressing objects: 100% (53/53), done.\u001b[K\n","remote: Total 105 (delta 52), reused 105 (delta 52), pack-reused 0\u001b[K\n","Receiving objects: 100% (105/105), 274.29 KiB | 3.97 MiB/s, done.\n","Resolving deltas: 100% (52/52), done.\n","/content/drive/MyDrive/15442/homework1\n"]}],"source":["# Code to set up the assignment\n","from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/\n","!mkdir -p 15442\n","%cd /content/drive/MyDrive/15442\n","!git clone https://github.com/mlsyscourse/homework1.git\n","%cd /content/drive/MyDrive/15442/homework1"]},{"cell_type":"markdown","metadata":{"id":"ITXgDFZSK_h5"},"source":["* If you are using local/server environment, please clone this repository.\n","\n","```shell\n","git clone https://github.com/mlsyscourse/homework1.git\n","cd homework1\n","export PYTHONPATH=.:$PYTHONPATH\n","```"]},{"cell_type":"markdown","metadata":{"id":"2eWqMr7QK_h5"},"source":["## Part 1: Automatic Differentiation Framework (60 pt)\n","\n","In part 1, you will implement the reverse mode automatic differentiation algorithm.\n","\n","The auto diff algorithm in this homework works on a **computational graph**.\n","A computational graph describes the process of computation of an expression.\n","For example, given $x_1$, $x_2$, the expression $y = x_1 \\times x_2 + x_1$ has the following computational graph:\n","\n","<img src=\"https://raw.githubusercontent.com/mlsyscourse/homework1/main/figure/computational_graph.jpg\" alt=\"figure/computational_graph.jpg\" width=\"60%\"/>"]},{"cell_type":"markdown","metadata":{"id":"MOLthvm3K_h6"},"source":["Let's first walk you through the basic concepts and data structures in the framework.\n","A computational graph consists of **nodes**, where each node denotes an intermediate step of computation during computing the entire expression.\n","Every node is composed of the three parts (`auto_diff.py` line 6):\n","\n","- an **operation** (field `op`), which defines the operation that the node computes.\n","- a list of **input nodes** (field `inputs`), which indicates the input source of the computation.\n","- optionally, additional \"**attributes**\" (field `attrs`). The attributes that a node has depends on the op of the node. We will explain the attributes later in this part.\n","\n","We can define an input node of a computational graph with `ad.Variable`. For example, the input variable nodes $x_1$ and $x_2$ can be defined as\n","\n","```python\n","import auto_diff as ad\n","\n","x1 = ad.Variable(name=\"x1\")\n","x2 = ad.Variable(name=\"x2\")\n","```\n","\n","In `auto_diff.py` (line 81), you can see that the essence of `ad.Variable` is to construct a node\n","with op `placeholder` and the given name. The input nodes have empty `inputs` and `attrs`:\n","```python\n","class Variable(Node):\n","    def __init__(self, name: str) -> None:\n","        super().__init__(inputs=[], op=placeholder, name=name)\n","```\n","\n","Here, the `placeholder` defines the computation of a input variable node, which is \"doing nothing.\"\n","Besides `placeholder`, we have other ops defined in `auto_diff.py`. For example,\n","\n","- op `add` defines the addition of two nodes,\n","- op `matmul` defines the matrix multiplication of two nodes.\n","\n","Notably, these ops are globally defined for only once, and the `op` field of every node is such\n","a globally defined op."]},{"cell_type":"markdown","metadata":{"id":"fAcv8S6NK_h6"},"source":["Now, back to our example of $y = x_1 \\times x_2 + x_1$.\n","Now that we have `x1` and `x2` as two input variable nodes, we can define the rest of the computational graph\n","with a one-line Python code:\n","```python\n","y = x1 * x2 + x1\n","```\n","\n","This line first constructs a node with op `mul` (multiplication) and `x1`, `x2` as `inputs`,\n","and then constructs a node with op `add` which takes the previous multiplication node and `x1` as `inputs`.\n","As a result, our computational graph contains four nodes in the end."]},{"cell_type":"markdown","metadata":{"id":"yYS8MfifK_h6"},"source":["It worths noting that a computational graph (e.g., the four nodes we defined) **does not** carry concrete values of nodes.\n","The style of this homework is consistent with the TensorFlow v1 style, introduced in the lecture.\n","This is different from frameworks like PyTorch, where the values of input tensors will be given in the beginning,\n","and the values of intermediate tensors are eagerly computed along the way when those tensors are defined.\n","In our computational graph, to compute the value of output `y` given values of input `x1`, `x2`,\n","we provide the `Evaluator` class (`auto_diff.py` line 373)."]},{"cell_type":"markdown","metadata":{"id":"lCip5YB7K_h6"},"source":["Here is an example of how `Evaluator` works. The constructor of `Evaluator` takes a list of nodes to evaluate.\n","By writing\n","```python\n","evaluator = ad.Evaluator(eval_nodes=[y])\n","```\n","it means that we construct an `Evaluator` instance which aims to compute the value of `y`.\n","Then we provide the values (assuming all `numpy.ndarray` in this homework) of the input tensors through the main interface `Evaluator.run` (which you need to implement):\n","```python\n","import numpy as np\n","\n","x1_value = np.array(2)\n","x2_value = np.array(3)\n","y_value = evaluator.run(input_dict={x1: x1_value, x2: x2_value})\n","```\n","\n","At a high level, here the `run` method consumes the input values via a dictionary `Dict[Node, numpy.ndarray]`,\n","computes the value of node `y` internally, and returns the result.\n","Given `2 * 3 + 2 = 8`, the returned `y_value` should be `np.ndarray(8)` eventually (of course, it will not return the correct value before your finish implementing):\n","```python\n","np.testing.assert_allclose(y_value, np.array(8))\n","```"]},{"cell_type":"markdown","metadata":{"id":"2LMnlNGTK_h7"},"source":["`Evaluator.run` method effectively computes the forward computation of nodes, and we can go ahead to talk about the backward.\n","As you learned in the lecture, in order to compute the output gradient with regard to each input node in a computational graph,\n","we can extend the forward graph with the additional backward part.\n","Once we have the forward and backward graph together, by given input node values,\n","we can use `Evaluator` to compute the output value, loss value, and the gradient values of each input nodes altogether with a single-time `Evaluator.run`.\n","\n","The function `gradients(output_node: Node, nodes: List[Node]) -> List[Node]` in `auto_diff.py` is\n","the function you need to implement to construct the backward graph.\n","This function takes an output node (usually the node of the loss function in machine learning), whose gradient is treated as 1,\n","takes the list of nodes to compute gradients for,\n","and returns the gradient nodes with regard to each node in the input list."]},{"cell_type":"markdown","metadata":{"id":"Sjz5vvUcK_h7"},"source":["Back to our example, after implementing `gradients`, you can run\n","```python\n","x1_grad, x2_grad = ad.gradients(output_node=y, node=[x1, x2])\n","```\n","to get the gradients of $y$ regarding $x_1$ and $x_2$ respectively.\n","And you can construct `Evaluator` on nodes `y`, `x1_grad` and `x2_grad`, and use `Evaluator.run`\n","to compute the output value and input gradients."]},{"cell_type":"markdown","metadata":{"id":"a9iDUNiYK_h7"},"source":["Finally, before leaving the homework to you, we introduce how op works.\n","As you can find in `auto_diff.py`, each op defines three methods:\n","\n","- `__call__(self, **kwargs) -> Node`, which takes in the input nodes (and attributes), constructs a new node with this op, and returns the constructed node.\n","- `compute(self, node: Node, input_values: List[np.ndarray]) -> np.ndarray`, which takes the node to compute with its input values, and returns the computed value of the given node.\n","- `gradient(self, node: Node, output_grad: Node) -> List[Node]`, which takes a node and the gradient node of this node, and returns the nodes of the input partial adjoints (one for each input node).\n","\n","In general, the `Op.compute` method computes the value of a single node with given node inputs, and `Evaluator.run` function computes the value of a graph output with given graph inputs.\n","`Op.gradient` method constructs the backward computational graph for a single node, and `gradients` function constructs the backward graph for a graph.\n","That being said, your implementation of `Evaluator.run` should effectively make use of the `compute` method of op,\n","and likewise, the `gradients` function implementation should leverage the `gradient` method defined in op."]},{"cell_type":"markdown","metadata":{"id":"moOEW6aNK_h7"},"source":["### Your tasks\n","\n","**Task 1 (10 pt).** Implement the `compute` method for all ops in `auto_diff.py`. We provide the examples of `AddOp` and `AddByConstOp`, and you need to implement the rest.\n","For this homework, you can assume that the inputs of addition/multiplication/division have the same shape.\n","\n","We provide sample tests in `tests/test_auto_diff_node_forward.py`.\n","You can test your task 1 implementation by running"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1146,"status":"ok","timestamp":1706414004471,"user":{"displayName":"Ruihang Lai","userId":"03605576729730874720"},"user_tz":300},"id":"13URNWeEK_h8","outputId":"a75d39e6-e114-455a-cbb8-44c1fc793ef8"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1m======================================= test session starts ========================================\u001b[0m\n","platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.3.0 -- /usr/bin/python3\n","cachedir: .pytest_cache\n","rootdir: /content/drive/MyDrive/15442/homework1\n","plugins: anyio-3.7.1\n","collected 8 items                                                                                  \u001b[0m\n","\n","tests/test_auto_diff_node_forward.py::test_mul \u001b[32mPASSED\u001b[0m\u001b[32m                                        [ 12%]\u001b[0m\n","tests/test_auto_diff_node_forward.py::test_mul_by_const \u001b[32mPASSED\u001b[0m\u001b[32m                               [ 25%]\u001b[0m\n","tests/test_auto_diff_node_forward.py::test_div \u001b[32mPASSED\u001b[0m\u001b[32m                                        [ 37%]\u001b[0m\n","tests/test_auto_diff_node_forward.py::test_div_by_const \u001b[32mPASSED\u001b[0m\u001b[32m                               [ 50%]\u001b[0m\n","tests/test_auto_diff_node_forward.py::test_matmul[False-False] \u001b[32mPASSED\u001b[0m\u001b[32m                        [ 62%]\u001b[0m\n","tests/test_auto_diff_node_forward.py::test_matmul[False-True] \u001b[32mPASSED\u001b[0m\u001b[32m                         [ 75%]\u001b[0m\n","tests/test_auto_diff_node_forward.py::test_matmul[True-False] \u001b[32mPASSED\u001b[0m\u001b[32m                         [ 87%]\u001b[0m\n","tests/test_auto_diff_node_forward.py::test_matmul[True-True] \u001b[32mPASSED\u001b[0m\u001b[32m                          [100%]\u001b[0m\n","\n","\u001b[32m======================================== \u001b[32m\u001b[1m8 passed\u001b[0m\u001b[32m in 0.31s\u001b[0m\u001b[32m =========================================\u001b[0m\n"]}],"source":["!python3 -m pytest -l -v tests/test_auto_diff_node_forward.py"]},{"cell_type":"markdown","metadata":{"id":"-x38qNHDK_h8"},"source":["**Task 2 (15 pt).** Implement the `Executor.run` method in `auto_diff.py`.\n","You may want to get the [topological sort](https://en.wikipedia.org/wiki/Topological_sorting) of the computational graph\n","in order to compute the output value.\n","\n","We provide sample tests in `tests/test_auto_diff_graph_forward.py`.\n","You can test your task 2 implementation by running"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":939,"status":"ok","timestamp":1706414006440,"user":{"displayName":"Ruihang Lai","userId":"03605576729730874720"},"user_tz":300},"id":"pP92pGx2K_h8","outputId":"9a034a75-19b5-4ef7-af62-1145c245bf6b"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1m======================================= test session starts ========================================\u001b[0m\n","platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.3.0 -- /usr/bin/python3\n","cachedir: .pytest_cache\n","rootdir: /content/drive/MyDrive/15442/homework1\n","plugins: anyio-3.7.1\n","collected 12 items                                                                                 \u001b[0m\n","\n","tests/test_auto_diff_graph_forward.py::test_identity \u001b[32mPASSED\u001b[0m\u001b[32m                                  [  8%]\u001b[0m\n","tests/test_auto_diff_graph_forward.py::test_add \u001b[32mPASSED\u001b[0m\u001b[32m                                       [ 16%]\u001b[0m\n","tests/test_auto_diff_graph_forward.py::test_add_by_const \u001b[32mPASSED\u001b[0m\u001b[32m                              [ 25%]\u001b[0m\n","tests/test_auto_diff_graph_forward.py::test_mul \u001b[32mPASSED\u001b[0m\u001b[32m                                       [ 33%]\u001b[0m\n","tests/test_auto_diff_graph_forward.py::test_mul_by_const \u001b[32mPASSED\u001b[0m\u001b[32m                              [ 41%]\u001b[0m\n","tests/test_auto_diff_graph_forward.py::test_div \u001b[32mPASSED\u001b[0m\u001b[32m                                       [ 50%]\u001b[0m\n","tests/test_auto_diff_graph_forward.py::test_div_by_const \u001b[32mPASSED\u001b[0m\u001b[32m                              [ 58%]\u001b[0m\n","tests/test_auto_diff_graph_forward.py::test_matmul[False-False] \u001b[32mPASSED\u001b[0m\u001b[32m                       [ 66%]\u001b[0m\n","tests/test_auto_diff_graph_forward.py::test_matmul[False-True] \u001b[32mPASSED\u001b[0m\u001b[32m                        [ 75%]\u001b[0m\n","tests/test_auto_diff_graph_forward.py::test_matmul[True-False] \u001b[32mPASSED\u001b[0m\u001b[32m                        [ 83%]\u001b[0m\n","tests/test_auto_diff_graph_forward.py::test_matmul[True-True] \u001b[32mPASSED\u001b[0m\u001b[32m                         [ 91%]\u001b[0m\n","tests/test_auto_diff_graph_forward.py::test_graph \u001b[32mPASSED\u001b[0m\u001b[32m                                     [100%]\u001b[0m\n","\n","\u001b[32m======================================== \u001b[32m\u001b[1m12 passed\u001b[0m\u001b[32m in 0.22s\u001b[0m\u001b[32m ========================================\u001b[0m\n"]}],"source":["!python3 -m pytest -l -v tests/test_auto_diff_graph_forward.py"]},{"cell_type":"markdown","metadata":{"id":"rZx-3v2EK_h8"},"source":["**Task 3 (15 pt).** Implement the `gradient` method for all ops in `auto_diff.py`. We provide the examples of `AddOp` and `AddByConstOp`, and you need to implement the rest.\n","\n","We provide sample tests in `tests/test_auto_diff_node_backward.py`.\n","You can test your task 3 implementation by running"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1376,"status":"ok","timestamp":1706414009774,"user":{"displayName":"Ruihang Lai","userId":"03605576729730874720"},"user_tz":300},"id":"cuFjQYnZK_h8","outputId":"414188a1-695a-4af3-cc68-4d88103fef16"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1m======================================= test session starts ========================================\u001b[0m\n","platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.3.0 -- /usr/bin/python3\n","cachedir: .pytest_cache\n","rootdir: /content/drive/MyDrive/15442/homework1\n","plugins: anyio-3.7.1\n","\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 7 items                                                                                  \u001b[0m\n","\n","tests/test_auto_diff_node_backward.py::test_mul \u001b[32mPASSED\u001b[0m\u001b[32m                                       [ 14%]\u001b[0m\n","tests/test_auto_diff_node_backward.py::test_div \u001b[32mPASSED\u001b[0m\u001b[32m                                       [ 28%]\u001b[0m\n","tests/test_auto_diff_node_backward.py::test_div_by_const \u001b[32mPASSED\u001b[0m\u001b[32m                              [ 42%]\u001b[0m\n","tests/test_auto_diff_node_backward.py::test_matmul[False-False] \u001b[32mPASSED\u001b[0m\u001b[32m                       [ 57%]\u001b[0m\n","tests/test_auto_diff_node_backward.py::test_matmul[False-True] \u001b[32mPASSED\u001b[0m\u001b[32m                        [ 71%]\u001b[0m\n","tests/test_auto_diff_node_backward.py::test_matmul[True-False] \u001b[32mPASSED\u001b[0m\u001b[32m                        [ 85%]\u001b[0m\n","tests/test_auto_diff_node_backward.py::test_matmul[True-True] \u001b[32mPASSED\u001b[0m\u001b[32m                         [100%]\u001b[0m\n","\n","\u001b[32m======================================== \u001b[32m\u001b[1m7 passed\u001b[0m\u001b[32m in 0.29s\u001b[0m\u001b[32m =========================================\u001b[0m\n"]}],"source":["!python3 -m pytest -l -v tests/test_auto_diff_node_backward.py"]},{"cell_type":"markdown","metadata":{"id":"C96znygCK_h9"},"source":["**Task 4 (20 pt).** Implement `gradients` function in `auto_diff.py`.\n","You may also find topological sort helpful in the implementation.\n","\n","We provide sample tests in `tests/test_auto_diff_graph_backward.py`.\n","You can test your task 4 implementation by running"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1190,"status":"ok","timestamp":1706414011383,"user":{"displayName":"Ruihang Lai","userId":"03605576729730874720"},"user_tz":300},"id":"sIPbNc0UK_h9","outputId":"cfd50c77-e18a-4c17-9c1e-d7ea0678ae83"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1m======================================= test session starts ========================================\u001b[0m\n","platform linux -- Python 3.10.12, pytest-7.4.4, pluggy-1.3.0 -- /usr/bin/python3\n","cachedir: .pytest_cache\n","rootdir: /content/drive/MyDrive/15442/homework1\n","plugins: anyio-3.7.1\n","\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 3 items                                                                                  \u001b[0m\n","\n","tests/test_auto_diff_graph_backward.py::test_graph \u001b[32mPASSED\u001b[0m\u001b[32m                                    [ 50%]\u001b[0m\n","tests/test_auto_diff_graph_backward.py::test_gradient_of_gradient \u001b[32mPASSED\u001b[0m\u001b[32m                     [100%]\u001b[0m\n","\n","\u001b[32m======================================== \u001b[32m\u001b[1m3 passed\u001b[0m\u001b[32m in 0.28s\u001b[0m\u001b[32m =========================================\u001b[0m\n"]}],"source":["!python3 -m pytest -l -v tests/test_auto_diff_graph_backward.py"]},{"cell_type":"markdown","metadata":{"id":"O-0ov5dha1Y3"},"source":["### A few notes\n","\n","1. **Zero-rank arrays in NumPy.** As mentioned earlier, all values are assumed to have type `numpy.ndarray` throughout this homework. One thing you may find interesting about NumPy is, if we add two zero-rank arrays together (e.g., `np.array(1) + np.array(2)`), it results in a scalar value, rather than a zero-rank array:\n","```\n",">>> x = np.array(1)\n",">>> y = np.array(2)\n",">>> type(x), type(y), x.ndim, y.ndim\n","(<class 'numpy.ndarray'>, <class 'numpy.ndarray'>, 0, 0)\n",">>> z = x + y\n",">>> z, type(z)\n","(3, <class 'numpy.int64'>)\n","```\n","This means that, if you want to have a rigorous implementation of your homework, you need to check the result type\n","at the end of `compute` methods, and wraps any scalar values back to `numpy.ndarray`.\n","However, for simplicity, we do not requrire you to do this, and it is completely up to you:\n","there will be no test for this behavior, and you won't get fewer credits because of not doing this.\n","Python by default also does not have eager type checking to throw error when you do not handle the scalars.\n","\n","2. **`Node.attrs`.** In the reference implementation of `AddByConstOp` in `auto_diff.py`, you will find that the `attrs` field is used to store the constant operand of the addition in the returned node. In general, the `attrs` field of a node stores all the **constants** that are known when constructing the computational graph: for the case of `AddByConstOp`, the constant operand is stored as a node attribute. While for general cases, an attribute does not have to be a node operand. You can see that in `MatMulOp` in `auto_diff.py`, we store the boolean flags denoting whether to transpose the input matrices as attributes. In the next part of this homework, you may implement op like `SumOp`, and find it useful to store the axis being reduced as a node attribute.\n","\n","3. **Minimality of `gradients`.** The `gradients` function constructs the backward graph and returns the gradient nodes with regards to required nodes. One interesting note here is the minimality of the constructed backward graph. For example, for a graph of `y = x1 * x2 + x1`, if we are only interested in the gradient of `x1 * x2`, a minimal backward graph only contains the gradient node of `x1 * x2`, which means it is not necessary to construct the gradient nodes for `x1` and `x2`. In this homework, we **do not** require you to construct the minimal backward graph, but it would be a good mental exercise to think about the possible pros/cons of constructing minimal backward graphs."]},{"cell_type":"markdown","metadata":{"id":"ZaOoXdqQyfKV"},"source":["## Part 2: SGD for logistic regression (40 pt)\n","\n","In this part, you need to implement the stochastic gradient descent (SGD) algorithm to train a simple logistic regression model.\n","\n","Specifically, for input $x\\in \\mathbb{R}^n$ , we'll consider a logistic regression model of the form\n","$$z = W^T x+b$$\n","where $W\\in \\mathbb{R}^{n\\times k}, b\\in \\mathbb{R}^k$ represent the weight and bias of modethe model, and $z\\in \\mathbb{R}^k$ represents the logits output by the network.\n","\n","The model should be trained with softmax / cross-entropy loss on mini-batches of training data,\n","which means we want to solve the following optimization problem, under the mini-batch setting.\n","\\begin{equation}\n","\\min_{W, b} \\;\\; \\ell_{\\mathrm{softmax}}(XW+b, y),\n","\\end{equation}\n","where $X\\in \\mathbb{R}^{b \\times n}$.\n"]},{"cell_type":"markdown","metadata":{"id":"U48mvZZQK_h9"},"source":["### Your tasks\n","\n","In general, you need the following steps (components) to train the logistic regression model:\n","\n","**Task 5 (15 pt).** Define the forward computational graph for $Z = XW+b$ in `logistic_regression` function in `logistic_regression.py`.\n","Note that $XW$ is a 2-dim matrix, while $b$ is a 1-dim vector.\n","You may find it helpful to introduce a new operator that broadcasts the $b$ vector to the matrix shape (think about why?).\n","In common frameworks, people use `broadcast_to`, e.g., [NumPy](https://numpy.org/doc/stable/reference/generated/numpy.broadcast_to.html),\n","for this purpose. However, our computational graph nodes do not carry shape information.\n","So you may want to slightly tweak the interface of your own broadcasting op."]},{"cell_type":"markdown","metadata":{"id":"0vBP4Qb3K_h-"},"source":["**Task 6 (15 pt).** Implement `softmax_loss` function in `logistic_regression.py` that constructs the computational graph of softmax loss.\n","The softmax loss takes an input node of logits and a node of one-hot encodings of the true labels.\n","As a reminder, for a multi-class output that can take on values $y \\in \\{1,\\ldots,k\\}$,\n","the softmax loss takes a vector of logits $z \\in \\mathbb{R}^k$ and the true class $y \\in \\{1,\\ldots,k\\}$ (which is encoded for this function as a one-hot vector),\n","and returns a loss defined by\n","\\begin{equation}\n","\\ell_{\\mathrm{softmax}}(z, y) = \\log\\sum_{i=1}^k \\exp z_i - z_y.\n","\\end{equation}\n","You may need to introduce new operators to compute summation, logarithm, exponentiation and their gradients to build up softmax loss function."]},{"cell_type":"markdown","metadata":{"id":"VjyrNstkK_h-"},"source":["**Task 7 (10 pt).** Implement `sgd_epoch` function in `logistic_regression.py` to run a single epoch of SGD.\n","In this function, you need to split the input data and labels into several mini-batches.\n","Then run the constructed computational graph given one batch as input.\n","Collect gradients and update the weight/bias of your logistic regression model correspondingly.\n","\n","If your implementation is correct, you will observe that the prediction accuracy on the handwritten digit dataset is around 95% by running `logistic_regression.py`:\n","```shell\n","> python3 logistic_regression.py\n","...\n","Final test accuracy: 0.9611111111111111\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!python3 logistic_regression.py"]},{"cell_type":"markdown","metadata":{"id":"E1vY5jDUK_h-"},"source":["**Hint.** When you find the current op set not satisfying your needs, consider introducing a new op."]},{"cell_type":"markdown","metadata":{"id":"WcGILNQ2K_h_"},"source":["## Part 3. Create Your Own Test Cases (0 pt)\n","\n","We encourage you to create your own test cases, which helps you confirm the correctness of your implementation.\n","If you are interested, you can write your own tests in `tests/test_customized_cases.py` and share them with us by including this file in your submission.\n","We appreciate it if you can share your tests, which can help improve this course and the homework. Please note that this part is voluntary."]},{"cell_type":"markdown","metadata":{"id":"LfTn0wD0K_h_"},"source":["## Part 4. Homework Feedback (0 pt)\n","\n","This is the first time we offer this course, and we appreciate any homework feedback from you.\n","You can leave your feedback (if any) in `feedback.txt`, and submit it together with the source code.\n","Possible choices can be:\n","\n","- How difficult do you think this homework is?\n","- How much time does the homework take? Which task takes the most time?\n","- Which part of the homework do you feel hard to understand?\n","- And any other things you would like to share.\n","\n","Your feedback will be very useful in helping us improve the homework quality\n","for next years.\n"]},{"cell_type":"markdown","metadata":{"id":"83n9F8uYK_h_"},"source":["## How to Submit Your Homework\n","\n","In the home directory for the assignment, execute the command"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!make handin.tar"]},{"cell_type":"markdown","metadata":{},"source":["This will create an archive file with `auto_diff.py`, `logistic_regression.py`, `tests/test_customized_cases.py` and `feedback.txt`.\n","You can check the contents of `handin.tar` to make sure it contains all the needed files:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!tar -tf handin.tar"]},{"cell_type":"markdown","metadata":{},"source":["It is expected to list the four files:\n","```\n","auto_diff.py\n","logistic_regression.py\n","feedback.txt\n","tests/test_customized_cases.py\n","```\n","\n","Then, please go to Autolab at https://autolab.andrew.cmu.edu/courses/15442-s24 and submit the file `handin.tar`.\n","\n","This assignment is not automatically graded, and you will not receive immediate feedback upon submission in Autolab.\n","You can submit multiple times, but only your final submission will be graded, and the time stamp of that submission will be used in determining any late penalties.\n","If you are enrolled in the course (on SIO), but not registered on Autolab and/or Gradescope, please let the course staff know in a private post on Piazza."]}],"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/mlsyscourse/homework1/blob/main/mlsys_hw1.ipynb","timestamp":1706413758241}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
